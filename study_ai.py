import pandas as pd
import yt_dlp
import re
import glob
from google import genai
import json
import time
import os
import html
import youtube_transcript_api as yta
from upload_supabase import URL, KEY
from supabase import create_client

supabase = create_client(URL, KEY)

# 1. é…ç½®
from dotenv import load_dotenv
load_dotenv()

# Read keys from env
env_keys = os.getenv("GOOGLE_API_KEYS", "")
API_KEYS = [k.strip() for k in env_keys.split(',') if k.strip()]
if not API_KEYS:
    # Fallback to empty list or handle error
    print("Warning: GOOGLE_API_KEYS not found in .env")

current_key_index = 0
client = None

def init_client():
    global client, current_key_index
    if not API_KEYS: return
    
    # Ensure index is valid
    current_key_index = current_key_index % len(API_KEYS)
    key = API_KEYS[current_key_index]
    
    client = genai.Client(api_key=key)
    print(f"ğŸ”‘ [ç³»çµ±] åˆ‡æ›è‡³ç¬¬ {current_key_index + 1} çµ„ API Key")

def rotate_key():
    global current_key_index
    current_key_index += 1
    init_client()

# Initialize first key
init_client()
MODEL_NAME = "gemini-3-flash-preview"

def clean_vtt_text(vtt_content):
    """
    ç°¡å–®æ¸…ç† VTT æ ¼å¼ï¼Œåªä¿ç•™æ–‡å­—ã€‚
    ç§»é™¤ header, timestamp, tag ç­‰ã€‚
    """
    lines = vtt_content.splitlines()
    text_lines = []
    # Regular expression for timestamp '00:00:00.000 --> 00:00:05.000'
    timestamp_pattern = re.compile(r'\d{2}:\d{2}:\d{2}\.\d{3}\s-->\s\d{2}:\d{2}:\d{2}\.\d{3}')
    
    seen_lines = set() # Avoid immediate duplicates often found in VTT karaoke-style
    
    # ç‹€æ…‹: è®€å– header ä¸­
    is_header = True

    for line in lines:
        line = line.strip()
        
        # Header filtering check for empty line
        if is_header and not line:
            is_header = False
            continue
            
        if not line: continue
        
        # Header filtering
        if is_header:
            if line == 'WEBVTT': continue
            if line.startswith('Kind:'): continue
            if line.startswith('Language:'): continue
            if line.startswith('Style:'): continue
            if line.startswith('::cue'): continue
            # If we see a timestamp, header is definitely over
            if timestamp_pattern.match(line) or '-->' in line:
                is_header = False
            # If we see normal text that is not a header key, maybe header is over?
            # Safest is to rely on timestamp or known keywords.
        
        if not line: continue
        if line.startswith('NOTE '): continue
        if timestamp_pattern.match(line): 
            is_header = False
            continue
        if '-->' in line: 
            is_header = False
            continue
        
        # Remove tags like <c.colorE6E6E6>...
        clean_line = re.sub(r'<[^>]+>', '', line)
        clean_line = html.unescape(clean_line)
        clean_line = clean_line.strip()
        
        if not clean_line: continue
        
        # Filter headers if they leaked (sometimes no timestamp before first cue if lazy?)
        if clean_line in ['WEBVTT', 'Kind: captions', 'Language: en']: continue
        
        # Simple dedup for adjacent lines
        if clean_line in seen_lines:
            pass
        else:
            text_lines.append(clean_line)
            seen_lines.add(clean_line)
            
    # Post-process to merge unique lines
    unique_lines = []
    last_line = ""
    for tl in text_lines:
        if tl != last_line:
            unique_lines.append(tl)
            last_line = tl
            
    return " ".join(unique_lines)

def parse_vtt_with_timestamps(vtt_content):
    """
    è§£æ VTT å…§å®¹ï¼Œå›å‚³ 'ç§’æ•¸|æ–‡å­—' æ ¼å¼ã€‚
    """
    lines = vtt_content.splitlines()
    output = []
    
    # Simple timestamp regex: 00:00:00.000
    timestamp_pattern = re.compile(r'(\d{2}):(\d{2}):(\d{2})\.(\d{3})')
    
    current_start = None
    
    for line in lines:
        line = line.strip()
        if not line: continue
        if line == 'WEBVTT': continue
        if '-->' in line:
            # Parse start time
            # 00:00:01.500 --> 00:00:03.000
            try:
                start_str = line.split('-->')[0].strip()
                match = timestamp_pattern.match(start_str)
                if match:
                    h, m, s, ms = map(int, match.groups())
                    current_start = h * 3600 + m * 60 + s
            except:
                pass
            continue
            
        # Skip tags/metadata
        if line.startswith('NOTE'): continue
        if line.startswith('Kind:'): continue
        if line.startswith('Language:'): continue
        
        # Text line
        if current_start is not None:
            # Remove HTML-like tags
            clean_text = re.sub(r'<[^>]+>', '', line)
            clean_text = html.unescape(clean_text).strip()
            if clean_text:
                output.append(f"{current_start}|{clean_text}")
                # Reset current_start so we don't repeat timestamp for multiple lines unless new timestamp appears? 
                # Actually VTT usually has timestamp then text. 
                # We keep current_start valid until next timestamp.
                
    return "\n".join(output)

def fetch_transcript_final(video_id):
    """
    å˜—è©¦ç²å–å¸¶æœ‰æ™‚é–“æˆ³è¨˜çš„å­—å¹•ã€‚
    å„ªå…ˆä½¿ç”¨ youtube_transcript_api (YTA)ï¼Œå¤±æ•—å‰‡é€€å› yt-dlp ä¸‹è¼‰ VTT è§£æã€‚
    """
    # 1. Try youtube_transcript_api
    try:
        print("   [å˜—è©¦] ä½¿ç”¨ youtube_transcript_api...")
        # Fix: YTA usage
        from youtube_transcript_api import YouTubeTranscriptApi
        
        try:
            # Method A: list_transcripts (Newer API, supports auto-generated)
            try:
                transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
                try:
                    transcript = transcript_list.find_transcript(['en', 'en-US', 'en-GB'])
                except:
                    # If specific english not found, get generated or any
                    try:
                        transcript = transcript_list.find_generated_transcript(['en'])
                    except:
                        transcript = next(iter(transcript_list))
                
                raw_data = transcript.fetch()
                
            except AttributeError:
                # Method B: Old API fallback
                print("    -> 'list_transcripts' not found, using 'get_transcript'...")
                raw_data = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'en-US'])

            full_text = ""
            for p in raw_data:
                start_time = int(p['start'])
                text = p['text'].replace('\n', ' ')
                full_text += f"{start_time}|{text}\n"
            
            print(f"   [æˆåŠŸ] YTA æŠ“å–å®Œæˆ ({len(full_text)} chars)")
            return full_text
        except Exception as inner_e:
            print(f"    -> YTA inner error: {inner_e}")
            raise inner_e

    except Exception as e:
        print(f"   [å¤±æ•—] YTA å¤±æ•—: {str(e)[:100]}... æ”¹ç”¨ yt-dlp")
        
    # 2. Fallback to yt-dlp
    try:
        url = f"https://www.youtube.com/watch?v={video_id}"
        temp_filename = f"temp_{video_id}"
        
        # Cleanup old files
        for f in glob.glob(f"{temp_filename}*"):
            try: os.remove(f)
            except: pass

        ydl_opts = {
            'skip_download': True,
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['en.*', 'en'], 
            'outtmpl': temp_filename,
            'quiet': True,
            'no_warnings': True,
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.extract_info(url, download=True)
            
        possible_files = glob.glob(f"{temp_filename}*.vtt")
        if not possible_files:
            print("   [å¤±æ•—] yt-dlp ä¹Ÿæ²’ä¸‹è¼‰åˆ°å­—å¹•")
            return None
            
        target_file = possible_files[0]
        # Prefer non-auto if available? Just pick first for now.
        
        with open(target_file, 'r', encoding='utf-8') as f:
            vtt_content = f.read()
            
        # Cleanup
        for f in possible_files:
            try: os.remove(f)
            except: pass
            
        print("   [æˆåŠŸ] yt-dlp ä¸‹è¼‰ä¸¦è®€å– VTT")
        return parse_vtt_with_timestamps(vtt_content)
        
    except Exception as e:
        print(f"   [å¤±æ•—] yt-dlp å¤±æ•—: {str(e)}")
        return None

def analyze_with_ai(text_with_timestamps):
    # 1. å®šç¾©æ‚¨çš„æ¨™æº–æ¨™ç±¤åº« (Standard Tag Library)
    tags_list = ["ç¤¾äº¤ (Social)", "è·å ´ (Work)", "æ—…éŠ (Travel)", "ç”Ÿæ´» (Daily)", "æ–‡åŒ– (Culture)", "å­¸è¡“ (Academic)"]

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‹±æ–‡è€å¸«ã€‚æˆ‘æœƒæä¾›ä¸€æ®µå½±ç‰‡é€å­—ç¨¿ï¼Œæ ¼å¼ç‚ºã€Œç§’æ•¸|è‹±æ–‡å…§å®¹ã€ã€‚
    
    è«‹åˆ†æä¸¦åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
    1. å¾é€™ä»½æ¨™ç±¤æ¸…å–®ä¸­ {tags_list}ï¼ŒæŒ‘é¸å‡º 1~2 å€‹æœ€ç¬¦åˆæœ¬å½±ç‰‡çš„æƒ…å¢ƒæ¨™ç±¤ã€‚
    2. æ•´ç†å‡º 8 å€‹æ ¸å¿ƒå–®å­— (é›£åº¦é©ä¸­ï¼Œå¯¦ç”¨ç‚ºä¸»)ã€‚
    3. æ•´ç†å‡º 6 å€‹å¸¸ç”¨å¥å‹ï¼Œä¸¦æ‰¾å‡ºè©²å¥å‹åœ¨å½±ç‰‡ä¸­å‡ºç¾çš„**æº–ç¢ºæ™‚é–“é» (timestamp)**ã€‚
    4. **æ‰€æœ‰è§£é‡‹èˆ‡ä¾‹å¥å¿…é ˆåŒ…å«ç¹é«”ä¸­æ–‡ç¿»è­¯**ã€‚
    
    å¿…é ˆåš´æ ¼éµå®ˆä»¥ä¸‹ JSON æ ¼å¼å›å‚³ (æ¬„ä½åç¨±è«‹ä¿æŒè‹±æ–‡)ï¼š
    {{
      "category": ["æ¨™ç±¤1", "æ¨™ç±¤2"],
      "vocabulary": [
        {{
          "word": "è‹±æ–‡å–®å­—",
          "phonetic": "KKéŸ³æ¨™",
          "definition": "è‹±æ–‡è§£é‡‹",
          "definition_zh": "ç¹é«”ä¸­æ–‡è§£é‡‹",
          "example": "è‹±æ–‡ä¾‹å¥",
          "example_zh": "ä¾‹å¥ä¸­æ–‡ç¿»è­¯"
        }}
      ],
      "sentence_patterns": [
        {{
          "structure": "å¥å‹çµæ§‹",
          "usage": "ç”¨æ³•èªªæ˜",
          "example": "ä¾‹å¥",
          "timestamp": 120  /* (æ•¸å­—, è©²å¥å‹åœ¨å½±ç‰‡ä¸­å‡ºç¾çš„ç§’æ•¸) */
        }}
      ]
    }}
    
    é€å­—ç¨¿å…§å®¹ (å‰ 6000 å­—)ï¼š
    {text_with_timestamps[:6000]}
    """
    response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
    raw_text = response.text.replace('```json', '').replace('```', '').strip()
    return json.loads(raw_text)

def main():
    # æª¢æŸ¥ API KEY æ˜¯å¦å·²è¨­å®š
    valid_keys = [k for k in API_KEYS if "YOUR_" not in k and "æ‚¨çš„" not in k and len(k) > 10]
    if not valid_keys:
        print("âŒ éŒ¯èª¤: è«‹å…ˆåœ¨ç¨‹å¼ç¢¼ç¬¬ 18 è¡Œå¡«å…¥è‡³å°‘ä¸€çµ„æœ‰æ•ˆçš„ Google API Key")
        return

    if not os.path.exists('links.csv'): return
    df = pd.read_csv('links.csv')
    urls = df.iloc[:, 0].tolist()
    if "http" in str(df.columns[0]): urls.insert(0, df.columns[0])

    results = []
    print(f"ğŸš€ ç”Ÿç”¢ç·šå•Ÿå‹•ï¼Œé è¨ˆè™•ç† {len(urls)} å€‹é€£çµ...")

    for i, url in enumerate(urls):
        v_id = str(url).split('v=')[-1].split('&')[0] if 'v=' in str(url) else str(url).split('/')[-1]
        print(f"[{i+1}/{len(urls)}] è™•ç†: {v_id}", end="")
        
        text = fetch_transcript_final(v_id)
        if text:
            # Retry loop for AI generation
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # å‚³é€çµ¦ AI åˆ†æ
                    analysis = analyze_with_ai(text)
                    
                    analysis['video_id'] = v_id
                    analysis['url'] = url
                    results.append(analysis)
                    
                    # Upload to Supabase immediately
                    try:
                        print(f"   [ä¸Šå‚³] æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
                        supabase.table('english_videos').upsert(analysis).execute()
                        print("   âœ… è³‡æ–™åº«æ›´æ–°æˆåŠŸï¼")
                    except Exception as db_err:
                        print(f"   âš ï¸ è³‡æ–™åº«å¯«å…¥å¤±æ•— (ä½†å·²å­˜å…¥ JSON): {db_err}")

                    print(" âœ… AIåˆ†ææˆåŠŸ")
                    break # Success, exit retry loop
                    
                except Exception as e:
                    error_str = str(e)
                    if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                        print(f" âš ï¸ é¡åº¦ç”¨ç›¡ (429)ï¼Œè‡ªå‹•åˆ‡æ›ä¸‹ä¸€çµ„ API Key...")
                        rotate_key()
                        time.sleep(2) # Brief pause before retry with new key
                    else:
                        print(f" âŒ AIåˆ†æå‡ºéŒ¯: {e}")
                        break # Other errors, don't retry
        else:
            print(" ğŸš« çœŸçš„æ‰¾ä¸åˆ°ä»»ä½•CCå­—å¹•è»Œ")
        
        time.sleep(10)

    with open('learning_data.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print("\nâœ¨ ä»»å‹™å®Œæˆï¼å·²ç”¢å‡ºç²¾è¯ç­†è¨˜ã€‚")

if __name__ == "__main__":
    main()